{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_API_TOKEN= os.getenv(\"HUGGINGFACE_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b5373372ec4abcaa47d97ab17f6b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7e92c3c0724415b350c26a47fc1359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6493799feac84e859043900113376807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98804ff2ff2d41738651bfd8f3cb122f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364ce9ad7c6845e2b490872b40e5addc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481c6ac5a60d4cac9aa90961dba189c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = ChatHuggingFace(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to Hindi.\"),\n",
    "    (\"human\", \"How are you?\"),\n",
    "]\n",
    "\n",
    "response = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "आप कैसे हैं?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/zeus/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model_kwargs = {\"max_length\":512}\n",
    "llm = HuggingFaceEndpoint(repo_id=repo_id, model_kwargs=model_kwargs, temperature=0.5, huggingfacehub_api_token=HUGGINGFACE_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm.invoke(\"How to generate Machine Learning Data Ingestion Pipeline?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Machine Learning (ML) data ingestion pipeline refers to the process of collecting, preprocessing, storing, and delivering data to ML models for training and inference. Building an effective ML data ingestion pipeline is crucial for ensuring that ML models have access to high-quality, timely, and relevant data. In this article, we will discuss the key steps involved in generating an ML data ingestion pipeline.\n",
       "\n",
       "1. Identify Data Sources:\n",
       "The first step in building an ML data ingestion pipeline is to identify the data sources that will be used to train and infer ML models. Data sources can include structured databases, unstructured data from files or streaming sources, or external APIs. It's essential to understand the data formats, access methods, and data volumes to ensure that the pipeline can handle the data efficiently.\n",
       "\n",
       "2. Data Collection:\n",
       "The next step is to collect the data from various sources. This can be done using different methods such as polling, web scraping, or using APIs. It's important to ensure that data collection is done in a scalable and reliable manner. Additionally, data should be collected in real-time or near-real-time to ensure that ML models are trained on the most recent data.\n",
       "\n",
       "3. Data Preprocessing:\n",
       "Once the data has been collected, it needs to be preprocessed to make it suitable for ML models. Preprocessing can include data cleaning, data transformation, feature engineering, and data normalization. Preprocessing is a critical step in the ML data ingestion pipeline as it can significantly impact the performance of ML models.\n",
       "\n",
       "4. Data Storage:\n",
       "The next step is to store the preprocessed data in a data lake or data warehouse. Data storage is an essential component of the ML data ingestion pipeline as it provides a central repository for storing large volumes of data. It's important to choose a data storage solution that can handle the data volumes, data formats, and access methods required by ML models.\n",
       "\n",
       "5. Data Delivery:\n",
       "The final step is to deliver the preprocessed data to ML models for training and inference. This can be done using different methods such as batch processing or real-time streaming. It's essential to ensure that data delivery is done in a scalable, reliable, and secure manner. Additionally, data delivery should be optimized to minimize latency and ensure that ML models have access to"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "    Question:{question}\n",
    "    Answer:Let's think step by step.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=['question'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] template=\"\\n    Question:{question}\\n    Answer:Let's think step by step.\\n\"\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:151: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm_chain.invoke(\"How to generate Machine Learning Data Ingestion Pipeline?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "    1. Data Collection: The first step is to collect data from various sources such as databases, APIs, files, or streaming sources. You can use tools like Apache Kafka, Apache Flume, or Apache NiFi for data collection.\n",
       "\n",
       "    2. Data Preprocessing: Once the data is collected, it needs to be preprocessed to make it suitable for machine learning models. This includes cleaning the data, transforming it into a format that can be used by machine learning algorithms, and handling missing values. Tools like Apache Spark, Apache Hadoop, or Apache Flink can be used for data preprocessing.\n",
       "\n",
       "    3. Data Storage: The preprocessed data needs to be stored in a data lake or data warehouse for further processing. Tools like Apache Hive, Apache Impala, or Apache HBase can be used for data storage.\n",
       "\n",
       "    4. Data Integration: The data from different sources needs to be integrated to create a unified dataset for machine learning models. Tools like Apache Beam or Apache Spark can be used for data integration.\n",
       "\n",
       "    5. Data Modeling: The next step is to apply machine learning algorithms to the data to build predictive models. Tools like TensorFlow, Scikit-learn, or Apache Spark MLlib can be used for data modeling.\n",
       "\n",
       "    6. Data Serving: The machine learning models need to be served to make predictions on new data. Tools like TensorFlow Serving, Flask, or Docker can be used for data serving.\n",
       "\n",
       "    7. Monitoring and Maintenance: The machine learning pipeline needs to be monitored and maintained to ensure that it is functioning correctly and providing accurate predictions. Tools like Prometheus, Grafana, or Nagios can be used for monitoring and maintenance.\n",
       "\n",
       "    8. Security: The machine learning pipeline needs to be secured to prevent unauthorized access and data breaches. Tools like Apache Ranger, Apache Knox, or Apache Sentry can be used for security.\n",
       "\n",
       "    9. Scalability: The machine learning pipeline needs to be scalable to handle large volumes of data and traffic. Tools like Apache Hadoop, Apache Spark, or Apache Kafka can be used for scalability.\n",
       "\n",
       "    10. Continuous Integration and Continuous Deployment (CI/CD): The machine learning pipeline needs to be deployed in a continuous integration and continuous deployment (CI/CD) environment to ensure"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(res['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
