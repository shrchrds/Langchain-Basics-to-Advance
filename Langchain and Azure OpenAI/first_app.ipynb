{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://huggingface.co/blog/keras-nlp-integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://huggingface.co/blog/keras-nlp-integration', 'title': 'Announcing New Hugging Face and Keras NLP integration', 'description': 'Weâ€™re on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nAnnouncing New Hugging Face and Keras NLP integration\\n\\n\\n\\n\\n\\n\\n\\n\\nHugging Face\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tModels\\n\\n\\t\\t\\t\\t\\tDatasets\\n\\n\\t\\t\\t\\t\\tSpaces\\n\\n\\t\\t\\t\\t\\tPosts\\n\\n\\t\\t\\t\\t\\tDocs\\n\\n\\n\\n\\n\\t\\t\\tSolutions\\n\\t\\t\\n\\nPricing\\n\\t\\t\\t\\n\\n\\n\\n\\n\\n\\nLog In\\n\\t\\t\\t\\t\\nSign Up\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tBack to Articles\\n\\n\\n\\n\\n\\n\\t\\tAnnouncing New Hugging Face and Keras NLP integration\\n\\t\\n\\n\\nPublished\\n\\t\\t\\t\\tJuly 10, 2024\\nUpdate on GitHub\\n\\n\\n\\t\\tUpvote\\n\\n\\t\\t25\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n+19\\n\\n\\n\\nariG23498\\nAritra Roy Gosthipaty\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse a wider range of frameworks\\n\\nHow to use it\\n\\nUnder the Hood: How It Works\\n\\nCommon Use Cases\\nGeneration\\n\\nChanging precision\\n\\nUsing the checkpoint with JAX backend\\n\\n\\nGemma 2\\n\\nPaliGemma\\n\\nWhat\\'s Next?\\n\\nThe Hugging Face Hub is a vast repository, currently hosting\\n750K+ public models,\\noffering a diverse range of pre-trained models for various machine\\nlearning frameworks. Among these,\\n346,268\\n(as of the time of writing) models are built using the popular\\nTransformers library.\\nThe KerasNLP library recently added an\\nintegration with the Hub compatible with a first batch of\\n33 models.\\nIn this first version, users of KerasNLP were limited to only the\\nKerasNLP-based models available on the Hugging Face Hub.\\nfrom keras_nlp.models import GemmaCausalLM\\n\\ngemma_lm = GemmaCausalLM.from_preset(\\n    \"hf://google/gemma-2b-keras\"\\n)\\n\\nThey were able to train/fine-tune the model and upload it back to\\nthe Hub (notice that the model is still a Keras model).\\nmodel.save_to_preset(\"./gemma-2b-finetune\")\\nkeras_nlp.upload_preset(\\n    \"hf://username/gemma-2b-finetune\",\\n    \"./gemma-2b-finetune\"\\n)\\n\\nThey were missing out on the extensive collection of over 300K\\nmodels created with the transformers library. Figure 1 shows 4k\\nGemma models in the Hub.\\n\\n\\n\\n\\n\\n\\n\\nFigure 1: Gemma Models in the Hugging Face Hub (Source:https://huggingface.co/models?other=gemma)\\n\\n\\n\\n\\n\\nHowever, what if we told you that you can now access and use these\\n300K+ models with KerasNLP, significantly expanding your model\\nselection and capabilities?\\n\\nfrom keras_nlp.models import GemmaCausalLM\\n\\ngemma_lm = GemmaCausalLM.from_preset(\\n    \"hf://google/gemma-2b\" # this is not a keras model!\\n)\\n\\nWe\\'re thrilled to announce a significant step forward for the NLP\\ncommunity: Transformers and KerasNLP now have a shared model save\\nformat. This means that models of the transformers library on the\\nHugging Face Hub can now also be loaded directly into KerasNLP - immediately\\nmaking a huge range of fine-tuned models available to KerasNLP users.\\nInitially, this integration focuses on enabling the use of\\nGemma (1 and 2), Llama 3, and PaliGemma models, with plans\\nto expand compatibility to a wider range of architectures in the near future.\\n\\n\\n\\n\\n\\n\\t\\tUse a wider range of frameworks\\n\\t\\n\\nBecause KerasNLP models can seamlessly use TensorFlow, JAX,\\nor PyTorch backends, this means that a huge range of model\\ncheckpoints can now be loaded into any of these frameworks in a single\\nline of code. Found a great checkpoint on Hugging Face, but you wish\\nyou could deploy it to TFLite for serving or port it into JAX to do\\nresearch? Now you can!\\n\\n\\n\\n\\n\\n\\t\\tHow to use it\\n\\t\\n\\nUsing the integration requires updating your Keras versions\\n$ pip install -U -q keras-nlp\\n$ pip install -U keras>=3.3.3\\n\\nOnce updated, trying out the integration is as simple as:\\nfrom keras_nlp.models import Llama3CausalLM\\n\\n# this model was not fine-tuned with Keras but can still be loaded\\ncausal_lm = Llama3CausalLM.from_preset(\\n    \"hf://NousResearch/Hermes-2-Pro-Llama-3-8B\"\\n)\\n\\ncausal_lm.summary()\\n\\n\\n\\n\\n\\n\\n\\t\\tUnder the Hood: How It Works\\n\\t\\n\\nTransformers models are stored as a set of config files in JSON format,\\na tokenizer (usually also a .JSON file), and a set of\\nsafetensors weights\\nfiles. The actual modeling code is contained in the Transformers\\nlibrary itself. This means that cross-loading a Transformers checkpoint\\ninto KerasNLP is relatively straightforward as long as both libraries\\nhave modeling code for the relevant architecture. All we need to do is\\nmap config variables, weight names, and tokenizer vocabularies from one\\nformat to the other, and we create a KerasNLP checkpoint from a\\nTransformers checkpoint, or vice-versa.\\nAll of this is handled internally for you, so you can focus on trying\\nout the models rather than converting them!\\n\\n\\n\\n\\n\\n\\t\\tCommon Use Cases\\n\\t\\n\\n\\n\\n\\n\\n\\n\\t\\tGeneration\\n\\t\\n\\nA first use case of language models is to generate text. Here is an\\nexample to load a transformers model and generate new tokens using\\nthe .generate method from KerasNLP.\\nfrom keras_nlp.models import Llama3CausalLM\\n\\n# Get the model\\ncausal_lm = Llama3CausalLM.from_preset(\\n    \"hf://NousResearch/Hermes-2-Pro-Llama-3-8B\"\\n)\\n\\nprompts = [\\n\"\"\"<|im_start|>system\\nYou are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\\n<|im_start|>user\\nWrite a short story about Goku discovering kirby has teamed up with Majin Buu to destroy the world.<|im_end|>\\n<|im_start|>assistant\"\"\",\\n]\\n\\n# Generate from the model\\ncausal_lm.generate(prompts, max_length=200)[0]\\n\\n\\n\\n\\n\\n\\n\\t\\tChanging precision\\n\\t\\n\\nYou can change the precision of your model using keras.config like so\\nimport keras\\nkeras.config.set_dtype_policy(\"bfloat16\")\\n\\nfrom keras_nlp.models import Llama3CausalLM\\n\\ncausal_lm = Llama3CausalLM.from_preset(\\n    \"hf://NousResearch/Hermes-2-Pro-Llama-3-8B\"\\n)\\n\\n\\n\\n\\n\\n\\n\\t\\tUsing the checkpoint with JAX backend\\n\\t\\n\\nTo test drive a model using JAX, you can leverage Keras to run the\\nmodel with the JAX backend. This can be achieved by simply switching\\nKeras\\'s backend to JAX. Hereâ€™s how you can use the model within the\\nJAX environment.\\nimport os\\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\\n\\nfrom keras_nlp.models import Llama3CausalLM\\n\\ncausal_lm = Llama3CausalLM.from_preset(\\n    \"hf://NousResearch/Hermes-2-Pro-Llama-3-8B\"\\n)\\n\\n\\n\\n\\n\\n\\n\\t\\tGemma 2\\n\\t\\n\\nWe are pleased to inform you that the Gemma 2 models are also\\ncompatible with this integration.\\nfrom keras_nlp.models import GemmaCausalLM\\n\\ncausal_lm = keras_nlp.models.GemmaCausalLM.from_preset(\\n    \"hf://google/gemma-2-9b\" # This is Gemma 2!\\n)\\n\\n\\n\\n\\n\\n\\n\\t\\tPaliGemma\\n\\t\\n\\nYou can also use any PaliGemma safetensor checkpoint in your KerasNLP pipeline.\\nfrom keras_nlp.models import PaliGemmaCausalLM\\n\\npali_gemma_lm = PaliGemmaCausalLM.from_preset(\\n    \"hf://gokaygokay/sd3-long-captioner\" # A finetuned version of PaliGemma\\n)\\n\\n\\n\\n\\n\\n\\n\\t\\tWhat\\'s Next?\\n\\t\\n\\nThis is just the beginning. We envision expanding this integration to\\nencompass an even wider range of Hugging Face models and architectures.\\nStay tuned for updates and be sure to explore the incredible potential\\nthat this collaboration unlocks!\\nI would like to take this opportunity to thank\\nMatthew Carrigan and\\nMatthew Watson for their\\nhelp in the entire process.\\n\\nMore Articles from our Blog\\n\\n\\nBlazing Fast SetFit Inference with ðŸ¤— Optimum Intel on Xeon\\n\\n\\t\\t\\t\\tBy\\xa0\\ndanielkorat\\n\\n\\nApril 3, 2024\\n\\nguest\\nâ€¢\\n\\n\\n\\t\\t\\t\\t7\\n\\nInteractively explore your Huggingface dataset with one line of code\\n\\n\\t\\t\\t\\tBy\\xa0\\nsps44\\n\\n\\nOctober 25, 2023\\n\\n\\n\\n\\t\\tUpvote\\n\\n\\t\\t25\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n+13\\n\\nCompany\\nÂ© Hugging Face\\nTOS\\nPrivacy\\nAbout\\nJobs\\n\\nWebsite\\nModels\\nDatasets\\nSpaces\\nPricing\\nDocs\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Announcing New Hugging Face and Keras NLP integration\\n\\n\\n\\n\\n\\n\\n\\n\\nHugging Face\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tModels\\n\\n\\t\\t\\t\\t\\tDatasets\\n\\n\\t\\t\\t\\t\\tSpaces\\n\\n\\t\\t\\t\\t\\tPosts\\n\\n\\t\\t\\t\\t\\tDocs\\n\\n\\n\\n\\n\\t\\t\\tSolutions\\n\\t\\t\\n\\nPricing\\n\\t\\t\\t\\n\\n\\n\\n\\n\\n\\nLog In\\n\\t\\t\\t\\t\\nSign Up\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tBack to Articles\\n\\n\\n\\n\\n\\n\\t\\tAnnouncing New Hugging Face and Keras NLP integration\\n\\t\\n\\n\\nPublished\\n\\t\\t\\t\\tJuly 10, 2024\\nUpdate on GitHub\\n\\n\\n\\t\\tUpvote\\n\\n\\t\\t25\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n+19\\n\\n\\n\\nariG23498\\nAritra Roy Gosthipaty\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse a wider range of frameworks\\n\\nHow to use it\\n\\nUnder the Hood: How It Works\\n\\nCommon Use Cases\\nGeneration\\n\\nChanging precision\\n\\nUsing the checkpoint with JAX backend\\n\\n\\nGemma 2\\n\\nPaliGemma\\n\\nWhat's Next?\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(model=\"text-embedding-ada-002\",api_version=\"2024-05-01-preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstoreDB = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Announcing New Hugging Face and Keras NLP integration\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Hugging Face\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\t\t\t\t\tModels\n",
       "\n",
       "\t\t\t\t\tDatasets\n",
       "\n",
       "\t\t\t\t\tSpaces\n",
       "\n",
       "\t\t\t\t\tPosts\n",
       "\n",
       "\t\t\t\t\tDocs\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\t\t\tSolutions\n",
       "\t\t\n",
       "\n",
       "Pricing\n",
       "\t\t\t\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Log In\n",
       "\t\t\t\t\n",
       "Sign Up\n",
       "\t\t\t\t\t\n",
       "\n",
       "\n",
       "\n",
       "\t\t\t\t\t\tBack to Articles\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\t\tAnnouncing New Hugging Face and Keras NLP integration\n",
       "\t\n",
       "\n",
       "\n",
       "Published\n",
       "\t\t\t\tJuly 10, 2024\n",
       "Update on GitHub\n",
       "\n",
       "\n",
       "\t\tUpvote\n",
       "\n",
       "\t\t25\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "+19\n",
       "\n",
       "\n",
       "\n",
       "ariG23498\n",
       "Aritra Roy Gosthipaty\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Use a wider range of frameworks\n",
       "\n",
       "How to use it\n",
       "\n",
       "Under the Hood: How It Works\n",
       "\n",
       "Common Use Cases\n",
       "Generation\n",
       "\n",
       "Changing precision\n",
       "\n",
       "Using the checkpoint with JAX backend\n",
       "\n",
       "\n",
       "Gemma 2\n",
       "\n",
       "PaliGemma\n",
       "\n",
       "What's Next?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "How to use it\n",
       "\n",
       "Under the Hood: How It Works\n",
       "\n",
       "Common Use Cases\n",
       "Generation\n",
       "\n",
       "Changing precision\n",
       "\n",
       "Using the checkpoint with JAX backend\n",
       "\n",
       "\n",
       "Gemma 2\n",
       "\n",
       "PaliGemma\n",
       "\n",
       "What's Next?\n",
       "\n",
       "The Hugging Face Hub is a vast repository, currently hosting\n",
       "750K+ public models,\n",
       "offering a diverse range of pre-trained models for various machine\n",
       "learning frameworks. Among these,\n",
       "346,268\n",
       "(as of the time of writing) models are built using the popular\n",
       "Transformers library.\n",
       "The KerasNLP library recently added an\n",
       "integration with the Hub compatible with a first batch of\n",
       "33 models.\n",
       "In this first version, users of KerasNLP were limited to only the\n",
       "KerasNLP-based models available on the Hugging Face Hub.\n",
       "from keras_nlp.models import GemmaCausalLM\n",
       "\n",
       "gemma_lm = GemmaCausalLM.from_preset(\n",
       "    \"hf://google/gemma-2b-keras\"\n",
       ")"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i,j in enumerate(documents[4:6]):\n",
    "    display(Markdown(documents[i].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Keras NLP?\"\n",
    "resp = vectorstoreDB.similarity_search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Use a wider range of frameworks\n",
       "\t\n",
       "\n",
       "Because KerasNLP models can seamlessly use TensorFlow, JAX,\n",
       "or PyTorch backends, this means that a huge range of model\n",
       "checkpoints can now be loaded into any of these frameworks in a single\n",
       "line of code. Found a great checkpoint on Hugging Face, but you wish\n",
       "you could deploy it to TFLite for serving or port it into JAX to do\n",
       "research? Now you can!\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\t\tHow to use it\n",
       "\t\n",
       "\n",
       "Using the integration requires updating your Keras versions\n",
       "$ pip install -U -q keras-nlp\n",
       "$ pip install -U keras>=3.3.3\n",
       "\n",
       "Once updated, trying out the integration is as simple as:\n",
       "from keras_nlp.models import Llama3CausalLM\n",
       "\n",
       "# this model was not fine-tuned with Keras but can still be loaded\n",
       "causal_lm = Llama3CausalLM.from_preset(\n",
       "    \"hf://NousResearch/Hermes-2-Pro-Llama-3-8B\"\n",
       ")\n",
       "\n",
       "causal_lm.summary()\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\t\tUnder the Hood: How It Works"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(resp[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on the following context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(model=\"gpt-4o\",api_version=\"2024-05-01-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1 = AzureChatOpenAI(model=\"gpt-35-turbo\",api_version=\"2024-05-01-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(llm1, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), config={'run_name': 'format_inputs'})\n",
       "| ChatPromptTemplate(input_variables=['context'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='\\n    Answer the following question based only on the following context:\\n    <context>\\n    {context}\\n    </context>\\n    '))])\n",
       "| AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fe9232841f0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fe923252dd0>, model_name='gpt-4o', openai_api_key=SecretStr('**********'), openai_proxy='', azure_endpoint='https://genaiexplorencus.openai.azure.com/', openai_api_version='2024-05-01-preview', openai_api_type='azure')\n",
       "| StrOutputParser(), config={'run_name': 'stuff_documents_chain'})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are some benefits of using KerasNLP models?'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain.invoke(\n",
    "    {\"input\":\"What is Keras NLP?\",\n",
    "    \"context\":[Document(page_content=\"Because KerasNLP models can seamlessly use TensorFlow, JAX, or PyTorch backends, this means that a huge range of model checkpoints can now be loaded into any of these frameworks in a single line of code. Found a great checkpoint on Hugging Face, but you wish you could deploy it to TFLite for serving or port it into JAX to do research? Now you can!\")]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstoreDB.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'AzureOpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fe9289e51b0>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='\\n    Answer the following question based only on the following context:\\n    <context>\\n    {context}\\n    </context>\\n    '))])\n",
       "            | AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fe9203bf7f0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fe9203bd1b0>, model_name='gpt-35-turbo', openai_api_key=SecretStr('**********'), openai_proxy='', azure_endpoint='https://genaiexplorencus.openai.azure.com/', openai_api_version='2024-05-01-preview', openai_api_type='azure')\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
